>> What is model inference?

   Inference is when we use the model to make predictions based on the processed input data.

>> In our example:

   1.Loading the Model: We need to load the pre-trained model for sentiment analysis:

python code:

'''
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
'''

   2. Passing Inputs through the Model: We feed the processed text (tensors) into the model:

python code:

'''
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
'''

The model outputs high-dimensional vectors (hidden states). The shape of these vectors is:

python code:

'''
torch.Size([2, 16, 768])
'''

   3. Model Heads: The model processes these vectors using a specific head for sequence classification, which reduces the dimensionality to match the number of labels:

python code:

'''
print(outputs.logits.shape)
'''

The shape of the output logits is:

python code:

'''
torch.Size([2, 2])
'''