Pretraining is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.