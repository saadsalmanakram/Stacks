# **Sliding Window Cache in ğŸ¤— Transformers**  

## **ğŸ“Œ What is Sliding Window Cache?**  
ğŸš€ **Sliding Window Cache (`sliding_window`)** is a **memory-efficient caching strategy** that:  
âœ” **Retains only the last `sliding_window` tokens** instead of storing the entire KV cache.  
âœ” **Works with models that support Sliding Window Attention**, e.g., **Mistral**.  
âœ” **Supports JIT optimizations**, making it **compatible with `torch.compile`**.  

ğŸ’¡ **Why is this useful?**  
- ğŸ† **Reduces GPU VRAM usage** by discarding old tokens in attention.  
- âš¡ **Ideal for long-context inference** without excessive memory growth.  
- ğŸ”¥ **Improves speed** by keeping memory requirements stable.  

---

## **ğŸš€ How Sliding Window Cache Works**  
1ï¸âƒ£ **Limits memory use** by storing **only the last `sliding_window` tokens**.  
2ï¸âƒ£ **Uses an attention window** instead of processing all past tokens.  
3ï¸âƒ£ **Ensures efficient long-text generation** with constant memory consumption.  
4ï¸âƒ£ **Compatible with JIT optimizations**, reducing runtime overhead.  

ğŸ“Œ **When to Use Sliding Window Cache?**  
- âœ… **Low GPU VRAM** and long sequences.  
- âœ… **Inference with models like Mistral**, which support **sliding window attention**.  
- âœ… **Long-context applications** such as **chatbots, summarization, or document processing**.  
- âŒ **Not supported for all models**â€”use only if your model has **sliding window attention**.  

---

## **ğŸ› ï¸ Enabling Sliding Window Cache**
To enable **Sliding Window Cache**, set:  
```python
cache_implementation="sliding_window"
```
in `generation_config` or directly in `generate()`.  

---

## **ğŸ’» Code Example: Using Sliding Window Cache**  
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load Mistral model & tokenizer
model_id = "mistralai/Mistral-7B-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16
).to("cuda:0")  # Load to GPU

# Define input prompt
inputs = tokenizer("Yesterday I was at a rock concert and", return_tensors="pt").to(model.device)

# Generate with Sliding Window Cache
out = model.generate(
    **inputs,
    do_sample=False,
    max_new_tokens=30,
    cache_implementation="sliding_window"  # Enable Sliding Window Cache
)

# Decode and print output
print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
```
ğŸ’¡ **Example Output:**  
```
Yesterday I was at a rock concert and it was amazing! The energy of the crowd, the lights, the soundâ€”everything was perfect. I danced all night...
```

---

## **ğŸ” Why Use Sliding Window Cache?**
âœ” **Reduces VRAM usage** by discarding older tokens.  
âœ” **Ideal for long sequences** without excessive memory growth.  
âœ” **Boosts efficiency** by keeping memory footprint stable.  
âœ” **Compatible with JIT compilation** for speed optimizations.  

---

## **ğŸ“Œ When to Use Sliding Window vs. Other Cache Types?**  

| **Cache Type**              | **Use Case** | **Best When** |
|----------------------------|-------------|--------------|
| **Static Cache** (`static`) | Pre-allocates KV cache for efficiency | **Long sequences, JIT optimization** |
| **Dynamic Cache** (`dynamic`) | Default, grows as needed | **Short or varying-length generations** |
| **Offloaded Cache** (`offloaded`) | Moves KV cache to CPU to save GPU memory | **Low VRAM, large models** |
| **Quantized Cache** (`quantized`) | Reduces memory via lower precision | **Long-context, memory-limited inference** |
| **Sliding Window Cache** (`sliding_window`) | Keeps only last N tokens in memory | **Long sequences, Mistral models** |

---

## **ğŸ”„ Combining Sliding Window Cache with JIT Compilation**
For further **speed optimizations**, you can combine **Sliding Window Cache** with `torch.compile`:  

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model
model_id = "mistralai/Mistral-7B-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    torch_dtype=torch.float16
).to("cuda:0")  # Load to GPU

# Compile model for faster execution
model = torch.compile(model)

# Define input
inputs = tokenizer("Once upon a time,", return_tensors="pt").to(model.device)

# Generate with Sliding Window Cache
out = model.generate(
    **inputs,
    do_sample=False,
    max_new_tokens=50,
    cache_implementation="sliding_window"
)

# Decode output
print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
```
---

## **ğŸ’¡ Summary: Best Practices**
âœ” **Use Sliding Window Cache for long-context models like Mistral**.  
âœ” **Ideal for inference with limited GPU VRAM**.  
âœ” **Prevents excessive memory growth in long-text generation**.  
âœ” **Combines well with JIT (`torch.compile`) for speed optimizations**.  

ğŸš€ **Sliding Window Cache enables long-sequence generation while maintaining efficient memory usage!**