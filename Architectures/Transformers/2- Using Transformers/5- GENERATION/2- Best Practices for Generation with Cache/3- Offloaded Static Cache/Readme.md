# **Offloaded Static Cache in ğŸ¤— Transformers**  

## **ğŸ“Œ What is Offloaded Static Cache?**  
ğŸš€ **Offloaded Static Cache (`offloaded_static`)** combines:  
âœ” **Offloading (moves KV cache to CPU)** to **reduce GPU memory usage**.  
âœ” **Static Cache (pre-allocates cache)** for **JIT optimizations** and **consistent memory usage**.  

ğŸ’¡ **Why is this useful?**  
- ğŸ† **Reduces VRAM usage** while still allowing for **JIT optimizations**.  
- âš¡ **Maintains fast generation speeds** by keeping cache access optimized.  
- ğŸ”¥ **Ideal for large models** where memory is a constraint but **performance is still a priority**.  

---

## **ğŸš€ How Offloaded Static Cache Works**  
1ï¸âƒ£ **Pre-allocates KV cache** like `StaticCache` to **avoid dynamic resizing**.  
2ï¸âƒ£ **Offloads KV cache to CPU**, reducing **GPU memory consumption**.  
3ï¸âƒ£ **Supports JIT optimizations** to improve execution speed.  
4ï¸âƒ£ **Maintains deterministic outputs** (unlike `QuantizedCache`, which can affect precision).  

ğŸ“Œ **When to Use Offloaded Static Cache?**  
- âœ… **Low GPU VRAM** but you still want **JIT optimizations**.  
- âœ… **Large model inference**, e.g., **LLaMA 2, Mistral, GPT-4-style models**.  
- âœ… **Long-context generation** where GPU memory is limited.  
- âŒ **Avoid if GPU VRAM is sufficient**, as it introduces **CPU-GPU transfer overhead**.  

---

## **ğŸ› ï¸ Enabling Offloaded Static Cache**
To enable **Offloaded Static Cache**, set:  
```python
cache_implementation="offloaded_static"
```
in `generation_config` or directly in `generate()`.  

---

## **ğŸ’» Code Example: Using Offloaded Static Cache**  
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load LLaMA 2 model & tokenizer
model_id = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Define input prompt
inputs = tokenizer("Hello, my name is", return_tensors="pt").to(model.device)

# Generate with Offloaded Static Cache
out = model.generate(
    **inputs,
    do_sample=False,
    max_new_tokens=20,
    cache_implementation="offloaded_static"  # Enable Offloaded Static Cache
)

# Decode and print output
print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
```
ğŸ’¡ **Example Output:**  
```
Hello, my name is John, and I am a software engineer with 10 years of experience in...
```

---

## **ğŸ” Why Use Offloaded Static Cache?**
âœ” **Reduces VRAM usage** by **offloading KV cache** to CPU.  
âœ” **Improves performance with JIT optimizations**.  
âœ” **Keeps deterministic outputs** (unlike quantized cache).  
âœ” **Prevents cache resizing overhead**, leading to **stable latency**.  

---

## **ğŸ“Œ When to Use Offloaded Static vs. Other Cache Implementations?**  

| **Cache Type**              | **Use Case** | **Best When** |
|----------------------------|-------------|--------------|
| **Static Cache** (`static`) | Pre-allocates KV cache for efficiency | **Long sequences, JIT optimization** |
| **Dynamic Cache** (`dynamic`) | Default, grows as needed | **Short or varying-length generations** |
| **Offloaded Cache** (`offloaded`) | Moves KV cache to CPU to save GPU memory | **Low VRAM, large models** |
| **Quantized Cache** (`quantized`) | Reduces memory via lower precision | **Long-context, memory-limited inference** |
| **Offloaded Static Cache** (`offloaded_static`) | Moves static KV cache to CPU | **Low VRAM + JIT optimizations** |

---

## **ğŸ”„ Combining Offloaded Static Cache with JIT Compilation**
For further **speed optimizations**, you can combine **Offloaded Static Cache** with `torch.compile`:  

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model
model_id = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    torch_dtype=torch.float16,
    device_map="auto"
)

# Compile model for faster execution
model = torch.compile(model)

# Define input
inputs = tokenizer("Once upon a time,", return_tensors="pt").to(model.device)

# Generate with Offloaded Static Cache
out = model.generate(
    **inputs,
    do_sample=False,
    max_new_tokens=50,
    cache_implementation="offloaded_static"
)

# Decode output
print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
```
---

## **ğŸ’¡ Summary: Best Practices**
âœ” **Use Offloaded Static Cache for memory savings** on **large models**.  
âœ” **Combine with JIT (`torch.compile`)** for performance boosts.  
âœ” **Ideal for large sequences when GPU VRAM is low**.  
âœ” **Avoid if you have enough VRAM**, as offloading introduces **CPU-GPU transfer latency**.  

ğŸš€ **Offloaded Static Cache enables large-model inference with reduced VRAM usage and faster execution!**