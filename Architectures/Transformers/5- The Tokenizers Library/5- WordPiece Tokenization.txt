>> What is WordPiece Tokenization?

   WordPiece is a method for breaking down words into smaller pieces, which was developed by Google for pretraining models like BERT. It helps the model understand words by splitting them into subwords or tokens. WordPiece is similar to Byte Pair Encoding (BPE) but has some differences in how tokens are combined and used.

>> Training Algorithm

   1. Initialization:
      - Start with a small vocabulary that includes special tokens and individual characters.
      - Example: For the word "word", the initial split would be `["w", "##o", "##r", "##d"]`.

   2. Merging Rules:
      - Learn merge rules by finding pairs of tokens that appear together often.
      - Instead of merging the most frequent pairs directly, WordPiece uses a scoring formula to prioritize less frequent token pairs:

     python code

     '''
     score = (freq_of_pair) / (freq_of_first_element * freq_of_second_element)
     '''

      - This formula helps merge pairs that are less common individually but appear together frequently.

   3. Example:
      - Given a vocabulary with pairs like `("hug", 10)` and `("pug", 5)`, the pairs are scored and merged based on the calculated scores.

>> Tokenization Algorithm

   1. Tokenization:
      - For a given word, find the longest subword that is in the vocabulary and split the word accordingly.
      - Example: For "hugs", the longest subword from the vocabulary is "hug", so it is split into `["hug", "##s"]`.

   2. Handling Unknown Words:
      - If a word can't be split into known subwords, it is marked as unknown.
      - Example: If "mug" is not in the vocabulary, it is tokenized as `["[UNK]"]`.

>> Implementing WordPiece Tokenization

   1. Prepare the Corpus:
      - Use the following corpus for demonstration:

     python code

     '''
     corpus = [
         "This is the Hugging Face Course.",
         "This chapter is about tokenization.",
         "This section shows several tokenizer algorithms.",
         "Hopefully, you will be able to understand how they are trained and generate tokens.",
     ]
     '''

   2. Pre-tokenize and Compute Frequencies:
      - Use a tokenizer to pre-tokenize the text and compute word frequencies:

     python code

     '''
     from transformers import AutoTokenizer
     from collections import defaultdict

     tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
     word_freqs = defaultdict(int)

     for text in corpus:
         words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
         new_words = [word for word, offset in words_with_offsets]
         for word in new_words:
             word_freqs[word] += 1
     '''

   3. Build the Initial Vocabulary:
      - Create a list of unique characters and special tokens:

     python code

     '''
     alphabet = []
     for word in word_freqs.keys():
         if word[0] not in alphabet:
             alphabet.append(word[0])
         for letter in word[1:]:
             if f"##{letter}" not in alphabet:
                 alphabet.append(f"##{letter}")

     alphabet.sort()
     vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
     '''

   4. Compute Pair Scores and Merge:
        - Define functions to compute pair scores and merge pairs:

     python code

     '''
     def compute_pair_scores(splits):
         letter_freqs = defaultdict(int)
         pair_freqs = defaultdict(int)
         for word, freq in word_freqs.items():
             split = splits[word]
             if len(split) == 1:
                 letter_freqs[split[0]] += freq
                 continue
             for i in range(len(split) - 1):
                 pair = (split[i], split[i + 1])
                 letter_freqs[split[i]] += freq
                 pair_freqs[pair] += freq
             letter_freqs[split[-1]] += freq

         scores = {
             pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
             for pair, freq in pair_freqs.items()
         }
         return scores

     def merge_pair(a, b, splits):
         for word in word_freqs:
             split = splits[word]
             if len(split) == 1:
                 continue
             i = 0
             while i < len(split) - 1:
                 if split[i] == a and split[i + 1] == b:
                     merge = a + b[2:] if b.startswith("##") else a + b
                     split = split[:i] + [merge] + split[i + 2 :]
                 else:
                     i += 1
             splits[word] = split
         return splits
     '''

   5. Train and Generate Vocabulary:
      - Continue merging pairs until reaching the desired vocabulary size:

     python code

     '''
     vocab_size = 70
     while len(vocab) < vocab_size:
         scores = compute_pair_scores(splits)
         best_pair, max_score = "", None
         for pair, score in scores.items():
             if max_score is None or max_score < score:
                 best_pair = pair
                 max_score = score
         splits = merge_pair(*best_pair, splits)
         new_token = (
             best_pair[0] + best_pair[1][2:]
             if best_pair[1].startswith("##")
             else best_pair[0] + best_pair[1]
         )
         vocab.append(new_token)
     '''

   6. Tokenize New Text:
      - Tokenize a new word by splitting it based on the learned vocabulary:

     python code
  
     '''
     def encode_word(word):
         tokens = []
         while len(word) > 0:
             i = len(word)
             # Implement the logic to find the longest subword in the vocabulary
     '''

This implementation illustrates how WordPiece tokenization is applied to a sample corpus, including building the vocabulary and tokenizing text.