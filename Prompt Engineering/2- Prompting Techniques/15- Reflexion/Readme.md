# Reflexion

## Overview
Reflexion is a framework designed to reinforce language-based agents through linguistic feedback. It introduces a paradigm shift by using verbal reinforcement, which helps agents iteratively improve their decision-making capabilities. The approach is based on self-reflection, where an agent assesses its past actions and refines its strategy accordingly.

Reflexion enhances the performance of agents in various advanced tasks, including sequential decision-making, programming, and reasoning. The method extends existing frameworks like ReAct and Chain-of-Thought (CoT) by integrating self-evaluation and explicit memory components.

---

## Key Components

### 1. Actor
The **Actor** is responsible for generating text-based actions based on the current state. It interacts with an environment, produces a trajectory, and receives observations. The Actor can be implemented using different models like:
- **Chain-of-Thought (CoT):** Enables step-by-step reasoning.
- **ReAct:** Merges reasoning and action-based responses.

A memory component is also introduced to store the history of actions and observations, providing context for future decisions.

### 2. Evaluator
The **Evaluator** assigns a score to the outputs generated by the Actor. This component can be implemented using:
- **LLMs** (Large Language Models): Used for heuristic-based evaluation.
- **Rule-Based Scoring:** Defines specific heuristics to judge the quality of outputs.

### 3. Self-Reflection
The **Self-Reflection** module generates linguistic feedback for the Actor, allowing it to refine its actions. This module leverages:
- **Reward Signals:** Helps in assessing performance.
- **Trajectory Analysis:** Evaluates past actions and their consequences.
- **Memory Storage:** Saves feedback for future improvement.

---

## Reflexion Workflow
1. **Define a Task:** Specify the goal that the Reflexion agent should accomplish.
2. **Generate a Trajectory:** The Actor takes actions and interacts with the environment.
3. **Evaluate Performance:** The Evaluator assigns a score based on performance metrics.
4. **Perform Self-Reflection:** The agent refines its approach based on linguistic feedback.
5. **Generate the Next Trajectory:** The improved strategy is applied in subsequent iterations.

This iterative learning cycle allows the agent to continuously improve over multiple attempts.

---

## Experimental Results
Reflexion has demonstrated significant improvements in various domains:

| Task | Baseline | Reflexion | Improvement |
|------|----------|----------|------------|
| Sequential Decision-Making (AlfWorld) | 105/134 tasks | 130/134 tasks | +23.8% |
| Reasoning (HotPotQA) | 72.5% | 85.2% | +12.7% |
| Programming (HumanEval, MBPP) | 67.3% | 79.1% | +11.8% |

---

## When to Use Reflexion?

Reflexion is ideal for scenarios where:
- Agents need to **learn from trial and error** (e.g., sequential decision-making).
- Traditional reinforcement learning (RL) methods are **impractical** due to high computational costs.
- **Nuanced feedback** is necessary instead of simple scalar rewards.
- **Interpretability and memory retention** are important.

### Example Use Cases:
- **Game AI:** Reflexion agents improve strategy over multiple iterations.
- **Automated Coding Assistants:** Reflexion helps models iteratively refine code outputs.
- **Scientific Reasoning:** The framework enhances logical deduction abilities.

---

## Code Examples

### 1. Implementing an Actor with Reflexion
```python
import openai

def reflexion_actor(prompt, memory=[]):
    """Generates actions based on past observations and memory."""
    context = "\n".join(memory) + "\n" + prompt
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "system", "content": "You are a Reflexion agent."},
                  {"role": "user", "content": context}]
    )
    return response["choices"][0]["message"]["content"]

# Example usage
memory = ["Observation: The code contains a syntax error.", "Feedback: Missing colon at line 3."]
prompt = "Generate a correct function definition for addition."
print(reflexion_actor(prompt, memory))
```

### 2. Implementing an Evaluator
```python
def evaluate_output(output, reference_answer):
    """Scores the output based on similarity to the reference answer."""
    return sum(1 for a, b in zip(output.split(), reference_answer.split()) if a == b) / len(reference_answer.split())

# Example usage
output = "def add(a, b): return a + b"
reference = "def add(a, b): return a + b"
print("Score:", evaluate_output(output, reference))
```

### 3. Implementing Self-Reflection
```python
def generate_self_reflection(trajectory, feedback):
    """Generates self-improvement suggestions based on trajectory feedback."""
    return f"Reviewing past mistakes: {trajectory}\nSuggested improvements: {feedback}"

# Example usage
trajectory = "Attempt 1: Code had syntax error. Attempt 2: Code lacked return statement."
feedback = "Ensure correct syntax and always include a return statement."
print(generate_self_reflection(trajectory, feedback))
```

---

## Limitations of Reflexion
While Reflexion is a powerful paradigm, it has some limitations:
- **Dependence on self-evaluation:** Reflexion relies on accurate self-assessment, which can be challenging for complex tasks.
- **Memory constraints:** Long-term memory is limited by context window size, requiring additional memory optimization techniques.
- **Code generation limitations:** Test-driven development approaches may not always specify accurate input-output mappings, particularly in dynamic environments.

---

## Future Work
- **Enhancing memory mechanisms** through vector embeddings or SQL databases.
- **Improving self-evaluation techniques** for more accurate reflection.
- **Expanding Reflexion to multimodal tasks** involving images and videos.

---

## Contributing
Contributions are welcome! Please follow these steps:
1. Fork the repository.
2. Create a new branch.
3. Commit changes with descriptive messages.
4. Submit a pull request.

---

## Research Papers to check
- Shinn et al. (2023). Reflexion: Reinforcement Through Verbal Feedback.
- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)
- [ReAct Framework](https://arxiv.org/abs/2210.03629)

---

