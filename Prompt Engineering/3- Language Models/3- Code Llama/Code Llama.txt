Code Llama is a group of large language models (LLMs) created by Meta. These models can handle text prompts and generate or discuss code. There are different versions of Code Llama (including one focused on Python and an "Instruct" variant) and different sizes: 7B, 13B, 34B, and 70B.

This guide will show you how to effectively use Code Llama for tasks like completing and debugging code. We will use the Code Llama 70B Instruct model (available on Together.ai), but you can choose other LLM providers. The results may vary depending on the provider, but the general prompts should work across platforms.

In this guide, we’ll cover how to:

- Access and configure the model
- Complete basic code tasks
- Debug code
- Write unit tests
- Generate Text-to-SQL queries
- Use "few-shot" prompting for complex tasks

1. Configuring Model Access

First, you need to set up the libraries required. Here’s how to do that:

```python
%%capture
!pip install openai
!pip install pandas
```

Next, import the necessary libraries and set up the Together.ai API key:

```python
import openai
import os
from dotenv import load_dotenv

load_dotenv()

TOGETHER_API_KEY = os.environ.get("TOGETHER_API_KEY")

client = openai.OpenAI(
    api_key=TOGETHER_API_KEY,
    base_url="https://api.together.xyz/v1",
)
```

To simplify using Code Llama, define a function for code completion:

```python
def get_code_completion(messages, max_tokens=512, model="codellama/CodeLlama-70b-Instruct-hf"):
    chat_completion = client.chat.completions.create(
        messages=messages,
        model=model,
        max_tokens=max_tokens,
        stop=["<step>"],
        temperature=0.7,
        n=10,
        frequency_penalty=1,
        presence_penalty=1,
        top_p=0.7,
    )
    return chat_completion
```

2. Basic Code Completion

Let’s try a simple example where we ask Code Llama to generate a Python function that calculates the nth Fibonacci number:

```python
messages = [
    {"role": "system", "content": "You are an expert programmer."},
    {"role": "user", "content": "Write a python function to generate the nth Fibonacci number."}
]

chat_completion = get_code_completion(messages)
print(chat_completion.choices[0].message.content)
```

You should get a function like this:

```python
def generate_fibonacci(n):
    if n == 1:
        return 0
    elif n == 2:
        return 1
    return generate_fibonacci(n-1) + generate_fibonacci(n-2)
```

3. Debugging Code

You can also use Code Llama to find bugs in code. For example, if you ask it to find the bug in this code:

```python
def fib(n):
    if n <= 0:
        return n
    else:
        return fib(n-1) + fib(n-2)
```

The model may point out that the function doesn't handle the case where `n` equals 1. It would suggest this fix:

```python
def fib(n):
    if n <= 0:
        return n
    elif n == 1:
        return 1
    else:
        return fib(n-1) + fib(n-2)
```

4. Writing Unit Tests

The model can help you write unit tests too. Here’s a prompt for checking a function that returns the unique elements in a list:

```python
prompt = """
[INST] Write 2 tests to check the correctness of a function that returns unique elements of a list. The tests should be between [TESTS] and [/TESTS] tags.
[/INST]
"""

messages = [
    {"role": "system", "content": "You are an expert programmer."},
    {"role": "user", "content": prompt},
]

chat_completion = get_code_completion(messages)
print(chat_completion.choices[0].message.content)
```

You might see tests like these:

```python
[TESTS]
# Test case 1:
assert get_unique_elements([1, 2, 3, 4, 5]) == [1, 2, 3, 4, 5]
# Test case 2:
assert get_unique_elements([1, 1, 2, 2, 3, 3]) == [1, 2, 3]
[/TESTS]
```

5. Text-to-SQL Generation

To generate SQL queries, you can prompt the model like this:

```python
prompt = """
Table departments, columns = [DepartmentId, DepartmentName]
Table students, columns = [DepartmentId, StudentId, StudentName]
Create a MySQL query for all students in the Computer Science Department.
"""

messages = [
    {"role": "user", "content": prompt}
]

chat_completion = get_code_completion(messages)
print(chat_completion.choices[0].message.content)
```

The result will be something like:

```sql
SELECT s.StudentId, s.StudentName
FROM students s
JOIN departments d ON s.DepartmentId = d.DepartmentId
WHERE d.DepartmentName = 'Computer Science';
```

6. Few-shot Prompting

For more complex tasks, you can use few-shot prompting. Here’s an example where you create a DataFrame and use a few examples to get specific results:

```python
import pandas as pd

data = {
    "Name": ["Alice", "Bob", "Carlos"],
    "Age": [20, 21, 22],
    "GPA": [3.8, 3.2, 3.5],
}

students_df = pd.DataFrame(data)
```

You can then ask Code Llama to find students with specific GPA ranges:

```python
prompt = "How to find students with GPAs between 3.5 and 3.8?"

messages = [
    {"role": "system", "content": "You are an expert with pandas."},
    {"role": "user", "content": prompt}
]

chat_completion = get_code_completion(messages)
print(chat_completion.choices[0].message.content)
```

The model should return code like:

```python
result = students_df[(students_df['GPA'] >= 3.5) & (students_df['GPA'] <= 3.8)]
```

With these simple steps, you can start using Code Llama effectively to assist with coding tasks, debugging, and generating SQL queries.